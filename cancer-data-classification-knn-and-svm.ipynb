{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5212576,"sourceType":"datasetVersion","datasetId":3032092}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T12:50:57.274943Z","iopub.execute_input":"2024-06-16T12:50:57.275501Z","iopub.status.idle":"2024-06-16T12:50:57.288865Z","shell.execute_reply.started":"2024-06-16T12:50:57.275464Z","shell.execute_reply":"2024-06-16T12:50:57.287557Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/cancer-data/Cancer_Data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd #managing the data\nimport numpy as np #mathematical operations\nimport matplotlib.pyplot as plt #vizualization\nimport seaborn as sns #visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:51:00.391391Z","iopub.execute_input":"2024-06-16T12:51:00.391810Z","iopub.status.idle":"2024-06-16T12:51:00.400384Z","shell.execute_reply.started":"2024-06-16T12:51:00.391779Z","shell.execute_reply":"2024-06-16T12:51:00.398669Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Load the data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/cancer-data/Cancer_Data.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:51:03.137760Z","iopub.execute_input":"2024-06-16T12:51:03.138175Z","iopub.status.idle":"2024-06-16T12:51:03.179921Z","shell.execute_reply.started":"2024-06-16T12:51:03.138144Z","shell.execute_reply":"2024-06-16T12:51:03.178582Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 33 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# About the data","metadata":{}},{"cell_type":"markdown","source":"\"Cancer Data\" dataset from Kaggle will be used. Dataset contains informations about cancer patients and their characteristics. \n\nThe dataset includes the following features:\n\n- ID: Patient ID;\n- diagnosis: Cancer types (B - benign cancer/M - malignant cancer);\n- radius_mean: Visual Characteristics of cancer;\n- texture_mean: Visual Characteristics of cancer;\n- perimeter_mean: Visual Characteristics of cancer;\n- area_mean: Visual Characteristics of cancer;\n- smoothness_mean: Visual Characteristics of cancer;\n- compactness_mean: Visual Characteristics of cancer;\n- concavity_mean: Visual Characteristics of cancer;\n- concave_points_mean: Visual Characteristics of cancer;\n- symmetry_mean;\n- fractial_dimension_mean;\n- radius_se;\n- texture_se;\n- perimeter_se;\n- area_se;\n- smoothness_se;\n- compactness_se;\n- concavity_se;\n- concave points_se;\n- symmetry_se;\n- fractal_dimension_se;\n- radius_worst;\n- texture_worst;\n- perimeter_worst;\n- area_worst;\n- smoothness_worst;\n- compactness_worst;\n- concavity_worst;\n- concave points_worst;\n- symmetry_worst;\n- fractal_dimension_worst;","metadata":{}},{"cell_type":"markdown","source":"# Objectives","metadata":{}},{"cell_type":"markdown","source":"The main task is to build a predictive model using the Cancer dataset from Kaggle to accurately classify whether a tumor is malignant or benign. We will train several ML models(KNN and SVM), and compare their results. \n\n\nK-Nearest Neighbors (KNN) classifies new cancer example based on the majority class of its k closest neighbors, while Support Vector Machine (SVM) finds the optimal hyperplane that best separates the data into different classes. Both models do well with binary problems.\n","metadata":{}},{"cell_type":"markdown","source":"# Data exploration and preprocessing","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:51:08.411514Z","iopub.execute_input":"2024-06-16T12:51:08.411937Z","iopub.status.idle":"2024-06-16T12:51:08.435858Z","shell.execute_reply.started":"2024-06-16T12:51:08.411906Z","shell.execute_reply":"2024-06-16T12:51:08.434675Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset contains 569 entries and 33 columns. We have only one object dtype, and we will use label encoding for that in the Feature Engineering part. Null values are present only in the \"Unnamed: 32\" column, so we will delete that column, as it is not relevant to our task and may cause us a problem in the future.\n","metadata":{}},{"cell_type":"code","source":"#droping the column \"Unnamed: 32 \"\ndata = data.drop('Unnamed: 32', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T11:57:44.166658Z","iopub.execute_input":"2024-06-16T11:57:44.167139Z","iopub.status.idle":"2024-06-16T11:57:44.176607Z","shell.execute_reply.started":"2024-06-16T11:57:44.167099Z","shell.execute_reply":"2024-06-16T11:57:44.174858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-16T11:57:47.158185Z","iopub.execute_input":"2024-06-16T11:57:47.158604Z","iopub.status.idle":"2024-06-16T11:57:47.169666Z","shell.execute_reply.started":"2024-06-16T11:57:47.158575Z","shell.execute_reply":"2024-06-16T11:57:47.168114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for duplicates \ndata.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T11:57:50.563589Z","iopub.execute_input":"2024-06-16T11:57:50.564202Z","iopub.status.idle":"2024-06-16T11:57:50.593076Z","shell.execute_reply.started":"2024-06-16T11:57:50.564155Z","shell.execute_reply":"2024-06-16T11:57:50.591229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent, no duplicates were found in the dataset.\n","metadata":{}},{"cell_type":"code","source":"data.diagnosis.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:17:37.648962Z","iopub.execute_input":"2024-06-15T13:17:37.649497Z","iopub.status.idle":"2024-06-15T13:17:37.661392Z","shell.execute_reply.started":"2024-06-15T13:17:37.649451Z","shell.execute_reply":"2024-06-15T13:17:37.660270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.diagnosis.value_counts().plot.bar(color=['green', 'red'])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:17:55.945919Z","iopub.execute_input":"2024-06-15T13:17:55.946319Z","iopub.status.idle":"2024-06-15T13:17:56.168000Z","shell.execute_reply.started":"2024-06-15T13:17:55.946287Z","shell.execute_reply":"2024-06-15T13:17:56.166922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that benign tumors are more common, even 0.6%, which is great news, because they are less dangerous, unlike malignant ones.\n","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Since the ML models we will use require numeric variables, we will use the label encoder to convert them from categorical to numeric values. Label encoder assigns a unique integer to each category, allowing the model to use these encoded values for training and prediction.\n","metadata":{}},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"# Get list of categorical variables\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:45:18.078497Z","iopub.execute_input":"2024-06-15T12:45:18.078910Z","iopub.status.idle":"2024-06-15T12:45:18.086327Z","shell.execute_reply.started":"2024-06-15T12:45:18.078879Z","shell.execute_reply":"2024-06-15T12:45:18.085178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data \nlabeled_data = data.copy()\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabeled_data[object_cols] = ordinal_encoder.fit_transform(data[object_cols])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:45:21.455892Z","iopub.execute_input":"2024-06-15T12:45:21.456307Z","iopub.status.idle":"2024-06-15T12:45:21.465453Z","shell.execute_reply.started":"2024-06-15T12:45:21.456273Z","shell.execute_reply":"2024-06-15T12:45:21.464029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labeled_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:45:54.185205Z","iopub.execute_input":"2024-06-15T12:45:54.186031Z","iopub.status.idle":"2024-06-15T12:45:54.216638Z","shell.execute_reply.started":"2024-06-15T12:45:54.185995Z","shell.execute_reply":"2024-06-15T12:45:54.215244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations checking","metadata":{}},{"cell_type":"markdown","source":"In order to get the most accurate model and to not overload it with not important informations, we want to keep only the most relevant features. We will filter the features and keep those that have a correlation greater than 0.5 with the target. Target in our case is column \"diagnosis\". For the same reason, for the most relevant features, we will check the presence of outliers and remove them.\n\n","metadata":{}},{"cell_type":"code","source":"#list of top features that have high correlation between features and target\nfeatures = labeled_data.corr()['diagnosis'].sort_values()\nfeatures","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:46:08.571526Z","iopub.execute_input":"2024-06-15T12:46:08.571933Z","iopub.status.idle":"2024-06-15T12:46:08.585111Z","shell.execute_reply.started":"2024-06-15T12:46:08.571902Z","shell.execute_reply":"2024-06-15T12:46:08.583512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_corr_features = features[features >= 0.5].index\nhigh_corr_features","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:48:42.872360Z","iopub.execute_input":"2024-06-15T12:48:42.872771Z","iopub.status.idle":"2024-06-15T12:48:42.881063Z","shell.execute_reply.started":"2024-06-15T12:48:42.872739Z","shell.execute_reply":"2024-06-15T12:48:42.879772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing high_corr_features to see if we have outliers\n\n# Ensure all selected columns are numeric\nnumeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n\n# Filter out non-numeric columns from high_corr_features\nhigh_corr_features = [col for col in high_corr_features if col in numeric_cols]\n\nfor column in high_corr_features:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=data[column])\n    plt.title(f'Box Plot for {column}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:54:56.674202Z","iopub.execute_input":"2024-06-15T12:54:56.674612Z","iopub.status.idle":"2024-06-15T12:54:59.678089Z","shell.execute_reply.started":"2024-06-15T12:54:56.674579Z","shell.execute_reply":"2024-06-15T12:54:59.677021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dealing with outliers - IQR","metadata":{}},{"cell_type":"code","source":"#perimeter_se column\n\n#first(q1) quartile, third(q3) quartile and interquartile range(iqr)\nq1 = data['perimeter_se'].quantile(0.25)\nq3 = data['perimeter_se'].quantile(0.75)\niqr = q3-q1\nq1, q3, iqr\n\n#upper and lower bounds for outliers\nupper_limit = q3 + (1.5 * iqr)\nlower_limit = q1 - (1.5 * iqr)\nlower_limit, upper_limit\n\n#data without outliers\nlabeled_data = labeled_data.loc[(labeled_data['perimeter_se'] < upper_limit) & (labeled_data['perimeter_se'] > lower_limit)]","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:07:12.361178Z","iopub.execute_input":"2024-06-15T13:07:12.361573Z","iopub.status.idle":"2024-06-15T13:07:12.372723Z","shell.execute_reply.started":"2024-06-15T13:07:12.361543Z","shell.execute_reply":"2024-06-15T13:07:12.371453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#area_se column\n\n#first(q1) quartile, third(q3) quartile and interquartile range(iqr)\nq1 = data['area_se'].quantile(0.25)\nq3 = data['area_se'].quantile(0.75)\niqr = q3-q1\nq1, q3, iqr\n\n#upper and lower bounds for outliers\nupper_limit = q3 + (1.5 * iqr)\nlower_limit = q1 - (1.5 * iqr)\nlower_limit, upper_limit\n\n#data without outliers\nlabeled_data = labeled_data.loc[(labeled_data['area_se'] < upper_limit) & (labeled_data['area_se'] > lower_limit)]","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:07:14.309881Z","iopub.execute_input":"2024-06-15T13:07:14.310936Z","iopub.status.idle":"2024-06-15T13:07:14.321930Z","shell.execute_reply.started":"2024-06-15T13:07:14.310901Z","shell.execute_reply":"2024-06-15T13:07:14.320563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#radius_se column\n\n#first(q1) quartile, third(q3) quartile and interquartile range(iqr)\nq1 = data['radius_se'].quantile(0.25)\nq3 = data['radius_se'].quantile(0.75)\niqr = q3-q1\nq1, q3, iqr\n\n#upper and lower bounds for outliers\nupper_limit = q3 + (1.5 * iqr)\nlower_limit = q1 - (1.5 * iqr)\nlower_limit, upper_limit\n\n#data without outliers\nlabeled_data = labeled_data.loc[(labeled_data['radius_se'] < upper_limit) & (labeled_data['radius_se'] > lower_limit)]","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:08:17.076727Z","iopub.execute_input":"2024-06-15T13:08:17.077113Z","iopub.status.idle":"2024-06-15T13:08:17.088638Z","shell.execute_reply.started":"2024-06-15T13:08:17.077082Z","shell.execute_reply":"2024-06-15T13:08:17.087537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I removed outliers from those features(perimeter_se, area_se, radius_se) where I concluded that there is the greatest deviation based on the box plot.","metadata":{}},{"cell_type":"markdown","source":"### Defining raw input and output","metadata":{}},{"cell_type":"code","source":"# Define X_raw as a copy of the data without the 'diagnosis' column\nX_raw = labeled_data[['area_se', 'perimeter_se', 'radius_se', 'compactness_worst',\n       'compactness_mean', 'concavity_worst', 'concavity_mean', 'area_mean',\n       'radius_mean', 'area_worst', 'perimeter_mean', 'radius_worst',\n       'concave points_mean', 'perimeter_worst', 'concave points_worst']]\n                  \n# Define y_raw as the 'diagnosis' column                  \ny = labeled_data['diagnosis']","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:16:19.493183Z","iopub.execute_input":"2024-06-15T13:16:19.494214Z","iopub.status.idle":"2024-06-15T13:16:19.503974Z","shell.execute_reply.started":"2024-06-15T13:16:19.494160Z","shell.execute_reply":"2024-06-15T13:16:19.502434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling Features","metadata":{}},{"cell_type":"markdown","source":"Scaling features ensures that all features contribute equally to the model and that the model can effectively learn from and generalize to new data. This preprocessing step is crucial for achieving optimal performance in classification tasks. Min-max scaler transforms features by scaling them to a specified range (0, 1).\n- Scaling is crucial for KNN model because it relies on distance metrics between feature vectors, and unscaled features can lead to biased influence from features with larger scales. \n- Scaling is important for SVM (Support Vector Machines) because it optimizes a decision boundary by maximizing the margin between classes, and unscaled features can cause the model to prioritize features with larger scales.\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Scaling the raw input features\nX = scaler.fit_transform(X_raw)\n\n#schecking what we done\nprint(f\"The range of feature inputs are within {X_scaled.min()} to {X_scaled.max()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:12:11.399254Z","iopub.execute_input":"2024-06-15T13:12:11.399724Z","iopub.status.idle":"2024-06-15T13:12:11.409687Z","shell.execute_reply.started":"2024-06-15T13:12:11.399678Z","shell.execute_reply":"2024-06-15T13:12:11.408484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## Spliting data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 123)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:12:17.608096Z","iopub.execute_input":"2024-06-15T13:12:17.608462Z","iopub.status.idle":"2024-06-15T13:12:17.618981Z","shell.execute_reply.started":"2024-06-15T13:12:17.608431Z","shell.execute_reply":"2024-06-15T13:12:17.617897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"markdown","source":"### Model training","metadata":{}},{"cell_type":"code","source":"# Defining a KNN model with `n_neighbors=2`\nknn_model = KNeighborsClassifier(n_neighbors=2)\n\nknn_model.fit(X_train, y_train.values.ravel())\npreds = knn_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:36:53.048945Z","iopub.execute_input":"2024-06-15T13:36:53.049329Z","iopub.status.idle":"2024-06-15T13:36:53.065284Z","shell.execute_reply.started":"2024-06-15T13:36:53.049296Z","shell.execute_reply":"2024-06-15T13:36:53.064215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluaton","metadata":{}},{"cell_type":"markdown","source":"- Accuracy: The proportion of correctly classified examples;\n- Precision: The proportion of true positive predictions among all positive predictions;\n- Recall: The proportion of true positive predictions among all actual positives;\n- F1 Score: Balance between precision and recall.\n","metadata":{}},{"cell_type":"code","source":"def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos\n\nevaluate_metrics(y_test, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:36:54.053243Z","iopub.execute_input":"2024-06-15T13:36:54.053645Z","iopub.status.idle":"2024-06-15T13:36:54.070958Z","shell.execute_reply.started":"2024-06-15T13:36:54.053612Z","shell.execute_reply":"2024-06-15T13:36:54.069280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning the number of K","metadata":{}},{"cell_type":"code","source":"# K number\nmax_k = 50\n# empty list to store f1score for each k\nf1_scores = []","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:24:31.939148Z","iopub.execute_input":"2024-06-15T13:24:31.939543Z","iopub.status.idle":"2024-06-15T13:24:31.944564Z","shell.execute_reply.started":"2024-06-15T13:24:31.939512Z","shell.execute_reply":"2024-06-15T13:24:31.943237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(1, max_k + 1):\n\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn = knn.fit(X_train, y_train.values.ravel())\n    preds = knn.predict(X_test)\n    \n    # Evaluate the model with f1score\n    f1 = f1_score(preds, y_test)\n    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n    \n# Convert the f1score list to a dataframe\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nf1_results.set_index('K')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:25:35.152038Z","iopub.execute_input":"2024-06-15T13:25:35.152487Z","iopub.status.idle":"2024-06-15T13:25:35.884243Z","shell.execute_reply.started":"2024-06-15T13:25:35.152398Z","shell.execute_reply":"2024-06-15T13:25:35.882822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that K=4 has the highest F1 Score, we will train the model with that value.\n","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=4)\nmodel.fit(X_train, y_train.values.ravel())\npreds = model.predict(X_test)\nevaluate_metrics(y_test, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:37:01.001940Z","iopub.execute_input":"2024-06-15T13:37:01.002336Z","iopub.status.idle":"2024-06-15T13:37:01.026163Z","shell.execute_reply.started":"2024-06-15T13:37:01.002306Z","shell.execute_reply":"2024-06-15T13:37:01.024966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"markdown","source":"### Model training","metadata":{}},{"cell_type":"code","source":"model = SVC(C=2, kernel='rbf')\nmodel.fit(X_train, y_train.values.ravel())\npreds = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:40:42.986748Z","iopub.execute_input":"2024-06-15T13:40:42.987694Z","iopub.status.idle":"2024-06-15T13:40:42.997884Z","shell.execute_reply.started":"2024-06-15T13:40:42.987615Z","shell.execute_reply":"2024-06-15T13:40:42.996752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos\n\nevaluate_metrics(y_test, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:40:44.306088Z","iopub.execute_input":"2024-06-15T13:40:44.306468Z","iopub.status.idle":"2024-06-15T13:40:44.320682Z","shell.execute_reply.started":"2024-06-15T13:40:44.306436Z","shell.execute_reply":"2024-06-15T13:40:44.319687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning C and kernel","metadata":{}},{"cell_type":"code","source":"params_grid = {\n    'C': [1, 10, 100, 150],\n    'kernel': ['poly', 'rbf', 'sigmoid']\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:39:44.826187Z","iopub.execute_input":"2024-06-15T13:39:44.826585Z","iopub.status.idle":"2024-06-15T13:39:44.832306Z","shell.execute_reply.started":"2024-06-15T13:39:44.826553Z","shell.execute_reply":"2024-06-15T13:39:44.831225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:39:46.139222Z","iopub.execute_input":"2024-06-15T13:39:46.139781Z","iopub.status.idle":"2024-06-15T13:39:46.144607Z","shell.execute_reply.started":"2024-06-15T13:39:46.139739Z","shell.execute_reply":"2024-06-15T13:39:46.143558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a GridSearchCV to search the best parameters\ngrid_search = GridSearchCV(estimator = model, \n                           param_grid = params_grid, \n                           scoring='f1',\n                           cv = 5, verbose = 1)\n\n# Search the best parameters with training data\ngrid_search.fit(X_train, y_train.values.ravel())\nbest_params = grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:39:47.185441Z","iopub.execute_input":"2024-06-15T13:39:47.185848Z","iopub.status.idle":"2024-06-15T13:39:47.755828Z","shell.execute_reply.started":"2024-06-15T13:39:47.185815Z","shell.execute_reply":"2024-06-15T13:39:47.754684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:39:55.590826Z","iopub.execute_input":"2024-06-15T13:39:55.591349Z","iopub.status.idle":"2024-06-15T13:39:55.599285Z","shell.execute_reply.started":"2024-06-15T13:39:55.591313Z","shell.execute_reply":"2024-06-15T13:39:55.598022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC(C=10, kernel='rbf')\nmodel.fit(X_train, y_train.values.ravel())\npreds = model.predict(X_test)\nevaluate_metrics(y_test, preds)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T13:37:44.451196Z","iopub.execute_input":"2024-06-15T13:37:44.451700Z","iopub.status.idle":"2024-06-15T13:37:44.474724Z","shell.execute_reply.started":"2024-06-15T13:37:44.451641Z","shell.execute_reply":"2024-06-15T13:37:44.473383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Key Findings","metadata":{}},{"cell_type":"markdown","source":"Among the models evaluated, the SVM classifier emerged as the best model with the highest accuracy, precision, recall, and F1 score. Here are the detailed metrics for the model:\n\n- Accuracy: 96%\n- Precision: 90%\n- Recall: 96%\n- F1 Score: 93%","metadata":{}},{"cell_type":"markdown","source":"### Future Work","metadata":{}},{"cell_type":"markdown","source":"- Additional Data: Adding data can help in training a more generalized model, reducing the risk of overfitting;\n- Alternative Models: Exploring ensemble methods or deep learning approaches can potentially improve performances;\n- Feature Engineering: Investigating additional features or transformations could provide better model accuracy.","metadata":{}}]}